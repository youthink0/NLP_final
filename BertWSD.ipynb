{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "183f7eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# select your GPU. Note that this should be set before you load tensorflow or pytorch.\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0,1,2,3'\n",
    "\n",
    "# To use multiple GPUs, combine all GPU ID with commas\n",
    "# e.g. >>> os.environ['CUDA_VISIBLE_DEVICES'] = '0,1,3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7e8b01a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "# Check if any GPU is used\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "833a4b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the model you want to use. Available models can be found here: https://huggingface.co/models\n",
    "MODEL_NAME = 'distilbert-base-uncased'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "ee0d01ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "bd700d96",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-073128ab64a99937\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset csv/default to /home/xiangyu/.cache/huggingface/datasets/csv/default-073128ab64a99937/0.0.0/6b9057d9e23d9d8a2f05b985917a0da84d70c5dae3d22ddd8a3f22fb01c69d9e...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb3818774c1c4005890d963b823bbe5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a75af7dfd83448783b20f5e6a499694",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset csv downloaded and prepared to /home/xiangyu/.cache/huggingface/datasets/csv/default-073128ab64a99937/0.0.0/6b9057d9e23d9d8a2f05b985917a0da84d70c5dae3d22ddd8a3f22fb01c69d9e. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f379ef04b0b4c47b1e1f71dda440165",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'sense'],\n",
      "        num_rows: 50\n",
      "    })\n",
      "})\n",
      "{'text': 'a naked [MASK], also known as nude [MASK], is a [MASK] where the participants are required to be nude.', 'sense': 1}\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset('csv', data_files = os.path.join('data', 'partytrain.csv'))\n",
    "print(dataset)\n",
    "print(dataset['train'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "0f0a40be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/distilbert-base-uncased/resolve/main/config.json from cache at /home/xiangyu/.cache/huggingface/transformers/23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.91b885ab15d631bf9cee9dc9d25ece0afd932f2f5130eba28f2055b2220c0333\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"distilbert-base-uncased\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/distilbert-base-uncased/resolve/main/vocab.txt from cache at /home/xiangyu/.cache/huggingface/transformers/0e1bbfda7f63a99bb52e3915dcf10c3c92122b827d92eb2d34ce94ee79ba486c.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
      "loading file https://huggingface.co/distilbert-base-uncased/resolve/main/tokenizer.json from cache at /home/xiangyu/.cache/huggingface/transformers/75abb59d7a06f4f640158a9bfcde005264e59e8d566781ab1415b139d2e4c603.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
      "loading file https://huggingface.co/distilbert-base-uncased/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/distilbert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/distilbert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xiangyu/.cache/huggingface/transformers/8c8624b8ac8aa99c60c912161f8332de003484428c47906d7ff7eb7f73eecdbb.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
      "loading configuration file https://huggingface.co/distilbert-base-uncased/resolve/main/config.json from cache at /home/xiangyu/.cache/huggingface/transformers/23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.91b885ab15d631bf9cee9dc9d25ece0afd932f2f5130eba28f2055b2220c0333\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"distilbert-base-uncased\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer # For tokenization\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "97b2b79e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# First, declare a new encoder\n",
    "encoder = OneHotEncoder(sparse = False)\n",
    "# Then, let the encoder learns all features in the given dataset\n",
    "# Keep in mind that all `fit` functions in sklearn only make the encoder learn from the data, not transforming the data yet.\n",
    "encoder = encoder.fit(np.reshape(dataset['train']['sense'], (-1, 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "104f76a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "LABEL_COUNT = len(encoder.categories_[0])\n",
    "print(LABEL_COUNT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "59b8cd05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([1, 2, 3])]\n"
     ]
    }
   ],
   "source": [
    "print(encoder.categories_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "0204e000",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(dataslice):\n",
    "    \"\"\" Input: a batch of your dataset\n",
    "        Example: { 'text': [['sentence1'], ['setence2'], ...],\n",
    "                   'sense': ['label1', 'label2', ...] }\n",
    "    \"\"\"\n",
    "    \n",
    "    # [ TODO ]\n",
    "    sense = [[i] for i in dataslice[\"sense\"]]\n",
    "    label = encoder.transform(sense)\n",
    "    output = tokenizer(dataslice[\"text\"])\n",
    "    output[\"labels\"] = label\n",
    "    \n",
    "    \n",
    "    \n",
    "    return output\n",
    "    \"\"\" Output: a batch of processed dataset\n",
    "        Example: { 'input_ids': ...,\n",
    "                   'attention_masks': ...,\n",
    "                   'label': ... }\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "19fbfc42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88252f7ce95c4ccfbab5af514acc3cd6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "processed_data = dataset.map(preprocess,    # your processing function\n",
    "                             batched = True # Process in batches so it can be faster\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "2d688ae4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['attention_mask', 'input_ids', 'labels', 'sense', 'text'],\n",
      "        num_rows: 50\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'attention_mask': [1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1],\n",
       " 'input_ids': [101,\n",
       "  1037,\n",
       "  6248,\n",
       "  103,\n",
       "  1010,\n",
       "  2036,\n",
       "  2124,\n",
       "  2004,\n",
       "  15287,\n",
       "  103,\n",
       "  1010,\n",
       "  2003,\n",
       "  1037,\n",
       "  103,\n",
       "  2073,\n",
       "  1996,\n",
       "  6818,\n",
       "  2024,\n",
       "  3223,\n",
       "  2000,\n",
       "  2022,\n",
       "  15287,\n",
       "  1012,\n",
       "  102],\n",
       " 'labels': [1.0, 0.0, 0.0],\n",
       " 'sense': 1,\n",
       " 'text': 'a naked [MASK], also known as nude [MASK], is a [MASK] where the participants are required to be nude.'}"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(processed_data)\n",
    "processed_data['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "91e1936b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "# declare a collator to do padding during traning.\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "7710be81",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/distilbert-base-uncased/resolve/main/config.json from cache at /home/xiangyu/.cache/huggingface/transformers/23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.91b885ab15d631bf9cee9dc9d25ece0afd932f2f5130eba28f2055b2220c0333\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"distilbert-base-uncased\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/distilbert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xiangyu/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.bias', 'vocab_transform.weight', 'vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'pre_classifier.weight', 'pre_classifier.bias', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Change to TFAutoModelForSequenceClassification if you're using tensoflow\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained('distilbert-base-uncased',\n",
    "                                                           num_labels = LABEL_COUNT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "9f80bd7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [ TODO ] Choose the validation data size                                v here\n",
    "train_val_dataset = processed_data['train'].train_test_split(test_size = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "35eea33d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['attention_mask', 'input_ids', 'labels', 'sense', 'text'],\n",
      "        num_rows: 45\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['attention_mask', 'input_ids', 'labels', 'sense', 'text'],\n",
      "        num_rows: 5\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Take a look at split data\n",
    "print(train_val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "bd44af02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change to TFTrainingArguments, TFTrainer if you're using tensoflow\n",
    "from transformers import TrainingArguments, Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "d003bc92",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "# [ TODO ] Set and tune your training properties\n",
    "LEARNING_RATE = 1e-5\n",
    "BATCH_SIZE = 8\n",
    "EPOCH = 300\n",
    "training_args = TrainingArguments(\n",
    "    output_dir = 'model',\n",
    "    learning_rate = LEARNING_RATE,\n",
    "    per_device_train_batch_size = BATCH_SIZE,\n",
    "    per_device_eval_batch_size = BATCH_SIZE,\n",
    "    num_train_epochs = EPOCH,\n",
    "    # You can also set other parameters here\n",
    ")\n",
    "\n",
    "# Now give all information to a trainer.\n",
    "trainer = Trainer(\n",
    "    model = model,\n",
    "    args = training_args,\n",
    "    train_dataset = train_val_dataset[\"train\"],\n",
    "    eval_dataset = train_val_dataset[\"test\"],\n",
    "    tokenizer = tokenizer,\n",
    "    data_collator = data_collator,\n",
    "    # You can also set other parameters\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "2d6c1cdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, sense.\n",
      "***** Running training *****\n",
      "  Num examples = 45\n",
      "  Num Epochs = 300\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 600\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='600' max='600' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [600/600 01:33, Epoch 300/300]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.075300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to model/checkpoint-500\n",
      "Configuration saved in model/checkpoint-500/config.json\n",
      "Model weights saved in model/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in model/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in model/checkpoint-500/special_tokens_map.json\n",
      "/home/xiangyu/anaconda3/envs/yolov5/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=600, training_loss=0.06620031992594401, metrics={'train_runtime': 93.2946, 'train_samples_per_second': 144.703, 'train_steps_per_second': 6.431, 'total_flos': 252878823004770.0, 'train_loss': 0.06620031992594401, 'epoch': 300.0})"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "543b3626",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in model/finetuned/config.json\n",
      "Model weights saved in model/finetuned/pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "model.save_pretrained(os.path.join('model', 'finetuned'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "517e19fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file model/finetuned/config.json\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"model/finetuned\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"problem_type\": \"multi_label_classification\",\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file model/finetuned/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing DistilBertForSequenceClassification.\n",
      "\n",
      "All the weights of DistilBertForSequenceClassification were initialized from the model checkpoint at model/finetuned.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use DistilBertForSequenceClassification for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "# Same, change to TFxxxxxx if you are using tensorflow\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "mymodel = AutoModelForSequenceClassification.from_pretrained(os.path.join('model', 'finetuned'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "0c74f963",
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = [\n",
    "    # 1\n",
    "    \"A naked [MASK], also known as nude [MASK], is a [MASK] where the participants are required to be nude.\",\n",
    "    # 2\n",
    "    \"he represents venstre, a danish centre-right [MASK].\",\n",
    "    # 3\n",
    "    \"in law, an allegation is a claim of a fact by a [MASK] in a pleading, charge, or defense.\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "e42e0928",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 3.9308, -4.0287, -3.9494],\n",
       "        [-4.4017,  4.2521, -4.2589],\n",
       "        [-2.8793, -3.0254,  3.0018]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Transform the sentences into embeddings\n",
    "input = tokenizer(examples, truncation=True, padding=True, return_tensors=\"pt\") # change return_tensors if youre using tensorflow\n",
    "# Get the output\n",
    "logits = mymodel(**input).logits\n",
    "logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "8f87fabf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[9.9927e-01, 3.4908e-04, 3.7786e-04],\n",
       "        [1.7439e-04, 9.9962e-01, 2.0116e-04],\n",
       "        [2.7774e-03, 2.3999e-03, 9.9482e-01]], grad_fn=<SoftmaxBackward>)"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Or `from tensorflow import nn` and `nn.softmax`\n",
    "from torch import nn\n",
    "\n",
    "predicts = nn.functional.softmax(logits, dim = -1)\n",
    "predicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "47d976af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def logits2labels(predicts):\n",
    "    _, labels = predicts.max(dim=1)\n",
    "    onehot = nn.functional.one_hot(labels, LABEL_COUNT)\n",
    "    return encoder.inverse_transform(onehot) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "124460bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-a7bfde7663847af0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset csv/default to /home/xiangyu/.cache/huggingface/datasets/csv/default-a7bfde7663847af0/0.0.0/6b9057d9e23d9d8a2f05b985917a0da84d70c5dae3d22ddd8a3f22fb01c69d9e...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5743bf8f8dd41e189982393a148baed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31d41812744542dd9a6312385958ea8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset csv downloaded and prepared to /home/xiangyu/.cache/huggingface/datasets/csv/default-a7bfde7663847af0/0.0.0/6b9057d9e23d9d8a2f05b985917a0da84d70c5dae3d22ddd8a3f22fb01c69d9e. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9dae2ae8aa84e09921a6728ee6314f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "testset = load_dataset('csv', data_files = os.path.join('data', 'partytest.csv'))\n",
    "\n",
    "# preprocess\n",
    "input = tokenizer(testset[\"train\"][\"text\"], truncation=True, padding=True, return_tensors=\"pt\")\n",
    "# get predictions\n",
    "logits = mymodel(**input).logits\n",
    "predicts = nn.functional.softmax(logits, dim = -1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "98c66470",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_label = logits2labels(predicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "e50ad568",
   "metadata": {},
   "outputs": [],
   "source": [
    "total = 0\n",
    "count = 0\n",
    "for P,G in zip(predict_label,  testset[\"train\"][\"sense\"]):\n",
    "    if P == G:\n",
    "        count += 1\n",
    "    total += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "dc13d6ee",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.893455098934551"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf9a94ac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
